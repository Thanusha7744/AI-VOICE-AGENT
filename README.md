# ğŸ™ï¸ AI Voice Agent - 30 Days Challenge (Day 1 to Day 13)

---

## ğŸ“Œ About the Project
This repository documents my journey through the **30 Days of AI Voice Agents Challenge**, where I built a complete **voice-based conversational AI** from scratch using **Python, FastAPI, Murf AI, AssemblyAI, and Google Gemini**.

The AI Voice Agent can:
- Listen to the user's **voice**
- Convert it into text using **Speech-to-Text (STT)**
- Process it with a **Large Language Model (LLM)**
- Reply back in a **natural AI-generated voice**
- Maintain **context across the conversation**
- Handle **errors gracefully** with fallback responses
- Offer a clean, responsive **web interface**

---

## ğŸ¯ Features Implemented (Days 1â€“13)
- **ğŸ¤ Voice Recording** â€” Record user audio from the browser
- **ğŸ—£ Text-to-Speech (TTS)** using Murf AI
- **ğŸ“ Speech-to-Text (STT)** using AssemblyAI
- **ğŸ§  AI Responses** using Google Gemini API
- **ğŸ”„ Full Voice-to-Voice Pipeline**
- **ğŸ’¬ Conversation Memory** (chat history)
- **âš  Error Handling** with fallback voice responses
- **ğŸ¨ UI Improvements** with interactive record button
- **ğŸ“š Documentation** explaining every step

---

## ğŸ›  Tech Stack

### **Backend**
- **Python 3.x**
- **FastAPI** â€” REST API framework
- **Murf AI API** â€” Text-to-Speech
- **AssemblyAI API** â€” Speech-to-Text
- **Google Gemini API** â€” LLM

### **Frontend**
- **HTML5** â€” Structure
- **CSS3** â€” Styling
- **JavaScript (ES6)** â€” API calls and logic
- **MediaRecorder API** â€” Voice recording
- **HTML `<audio>` element** â€” Audio playback

### **Tools**
- **Postman / FastAPI Docs** â€” API testing
- **.env** â€” API key security
- **Git & GitHub** â€” Version control

---

## ğŸ“‚ Folder Structure

voice-agent/
â”‚
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ main.py # FastAPI backend with endpoints
â”‚ â”œâ”€â”€ requirements.txt # Python dependencies
â”‚ â”œâ”€â”€ .env # API keys (ignored in GitHub)
â”‚ â”œâ”€â”€ uploads/ # Temporary audio storage
â”‚ â””â”€â”€ utils/
â”‚ â”œâ”€â”€ tts_service.py # Murf API TTS logic
â”‚ â”œâ”€â”€ stt_service.py # AssemblyAI STT logic
â”‚ â”œâ”€â”€ llm_service.py # Gemini API logic
â”‚ â””â”€â”€ chat_history.py # Conversation memory logic
â”‚
â”œâ”€â”€ frontend/
â”‚ â”œâ”€â”€ index.html # Main UI
â”‚ â”œâ”€â”€ script.js # Recording & API interaction
â”‚ â”œâ”€â”€ styles.css # UI styling
â”‚
â”œâ”€â”€ README.md # Documentation
â””â”€â”€ .gitignore # Ignore sensitive files



---

## ğŸ“… Day-by-Day Development Journey

### **Day 1: Project Setup**
- Set up **FastAPI** backend.
- Created `index.html` and `script.js`.
- Served HTML from backend.
- Verified server runs locally.

---

### **Day 2: Your First REST TTS Call**
- Created `/generate` endpoint.
- Integrated **Murf AI API** for Text-to-Speech.
- Returned generated audio URL.
- Tested with FastAPI docs & Postman.

---

### **Day 3: Playing Back TTS Audio**
- Added text input + submit button.
- On submit â†’ sent text to `/generate`.
- Played audio from returned URL in `<audio>`.

---

### **Day 4: Echo Bot v1**
- Implemented **MediaRecorder API**.
- Added "Start" and "Stop" recording buttons.
- Played back recorded audio locally.

---

### **Day 5: Send Audio to the Server**
- Built `/upload` endpoint.
- Uploaded audio file from frontend to backend.
- Saved files temporarily in `/uploads`.
- Returned metadata (name, type, size).

---

### **Day 6: Server-Side Transcription**
- Integrated **AssemblyAI** STT API.
- Created `/transcribe/file` endpoint.
- Sent audio directly for transcription.
- Displayed transcription in UI.

---

### **Day 7: Echo Bot v2**
- Created `/tts/echo` endpoint.
- Flow: Audio â†’ STT â†’ Murf TTS â†’ Play AI voice.
- First time replacing recorded audio with AI voice.

---

### **Day 8: Integrating an LLM**
- Added `/llm/query` endpoint.
- Sent text to **Google Gemini API**.
- Returned AI-generated text.

---

### **Day 9: Full Non-Streaming Pipeline**
- Updated `/llm/query` to accept **audio input**.
- Flow: Audio â†’ STT â†’ LLM â†’ TTS â†’ Play.
- Achieved **voice-to-voice AI conversation**.

---

### **Day 10: Chat History**
- Created `/agent/chat/{session_id}` endpoint.
- Stored messages in a session-based dictionary.
- Allowed multi-turn conversations with context.
- Session ID stored in frontend URL.

---

### **Day 11: Error Handling**
- Added backend try-except for all API calls.
- Added frontend error handling for API failures.
- Fallback voice: *"I'm having trouble connecting right now."*

---

### **Day 12: UI Revamp**
- Removed old Echo Bot and TTS demo sections.
- Kept only conversational bot interface.
- Combined start/stop into one button with animations.
- Improved styling for better UX.

---

### **Day 13: Documentation**
- Created **README.md** with:
  - Project overview
  - Features
  - Tech stack
  - Folder structure
  - Detailed day-by-day progress
  - Setup instructions

---

## âš™ How to Run Locally

### 1ï¸âƒ£ - Clone the Repository

```bash
git clone https://github.com/<your-username>/<repo-name>.git
cd <repo-name>

2ï¸âƒ£ - Install Backend Dependencies

cd backend
pip install -r requirements.txt

3ï¸âƒ£ - Create .env File

MURF_API_KEY=your_murf_api_key
ASSEMBLYAI_API_KEY=your_assemblyai_api_key
GEMINI_API_KEY=your_gemini_api_key

4ï¸âƒ£ - Start Backend Server
uvicorn main:app --reload

5ï¸âƒ£ - Open Frontend
Open frontend/index.html in browser.
Or serve it via FastAPI static file serving.

 ARCHITECTURE FLOW: 
 
User Speaks (Voice)
      â†“
[Frontend] MediaRecorder API
      â†“
Send Audio to Backend
      â†“
[Backend - FastAPI]
  â†’ AssemblyAI (Speech-to-Text)
  â†’ Google Gemini (LLM)
  â†’ Murf AI (Text-to-Speech)
      â†“
Return Audio URL
      â†“
[Frontend] Play AI Voice in <audio>